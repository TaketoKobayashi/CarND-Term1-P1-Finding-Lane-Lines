
# Self-Driving Car Engineer Nanodegree


## Project: **Finding Lane Lines on the Road** 
***
1.Load 1 image
2.Apply grayscale()
3.Apply canny()
4.Apply region_on_interest()
5.Apply draw_lines()
6.Apply hough_lines()

---

## Import Packages


```python
#importing some useful packages
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import numpy as np
import cv2
import os
from moviepy.editor import VideoFileClip
%matplotlib inline
```

## Read in an Image


```python
#reading in an image
image = mpimg.imread('test_images/solidWhiteRight.jpg')

#printing out some stats and plotting
print('This image is:', type(image), 'with dimesions:', image.shape)
plt.imshow(image)  # if you wanted to show a single color channel image called 'gray', for example, call as plt.imshow(gray, cmap='gray')
```

    This image is: <class 'numpy.ndarray'> with dimesions: (540, 960, 3)
    




    <matplotlib.image.AxesImage at 0xa361940>




![png](output_4_2.png)


## Ideas for Lane Detection Pipeline

**Some OpenCV functions (beyond those introduced in the lesson) that might be useful for this project are:**

`cv2.inRange()` for color selection  
`cv2.fillPoly()` for regions selection  
`cv2.line()` to draw lines on an image given endpoints  
`cv2.addWeighted()` to coadd / overlay two images
`cv2.cvtColor()` to grayscale or change color
`cv2.imwrite()` to output images to file  
`cv2.bitwise_and()` to apply a mask to an image

**Check out the OpenCV documentation to learn about these and discover even more awesome functionality!**

## Helper Functions

Below are some helper functions to help get you started. They should look familiar from the lesson!


```python
import math

def grayscale(img):
    """Applies the Grayscale transform
    This will return an image with only one color channel
    but NOTE: to see the returned image as grayscale
    (assuming your grayscaled image is called 'gray')
    you should call plt.imshow(gray, cmap='gray')"""
    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    # Or use BGR2GRAY if you read an image with cv2.imread()
    # return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    
def canny(img, low_threshold, high_threshold):
    """Applies the Canny transform"""
    return cv2.Canny(img, low_threshold, high_threshold)

def gaussian_blur(img, kernel_size):
    """Applies a Gaussian Noise kernel"""
    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)

def region_of_interest(img, vertices):
    """
    Applies an image mask.
    
    Only keeps the region of the image defined by the polygon
    formed from `vertices`. The rest of the image is set to black.
    """
    #defining a blank mask to start with
    mask = np.zeros_like(img)   
    
    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image
    if len(img.shape) > 2:
        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image
        ignore_mask_color = (255,) * channel_count
    else:
        ignore_mask_color = 255
        
    #filling pixels inside the polygon defined by "vertices" with the fill color    
    cv2.fillPoly(mask, vertices, ignore_mask_color)
    
    #returning the image only where mask pixels are nonzero
    masked_image = cv2.bitwise_and(img, mask)
    return masked_image


def draw_lines(img, lines, color=[255, 0, 0], thickness=10):
    # For calculating avg x-axis values for lines
    mid_x_val = 485           # middle x-point diveds left and right lines
    left_btm = []             # x-vertices for LEFT bottom part of image region
    left_top = []             # x-vertices for LEFT top part of image region
    right_btm = []            # x-vertices for RIGHT bottom part of image region
    right_top = []            # x-vertices for RIGHT top part of image region
    
    for line in lines:
        for x1,y1,x2,y2 in line:
            #cv2.line(img, (x1, y1), (x2, y2), color, thickness)
            # # x-extrapolation formula:
            # x = x1 + ((y-y1)*(x2-x1))/(y2-y1)
            
            # Preventing case with dividing by 0
            if y1 == y2: break
                
            # Apply x-extrapolation
            y_btm = img.shape[0]
            x_btm = int(round(x1 + ((y_btm-y1)*(x2-x1))/(y2-y1)))
            y_top = 320
            x_top = int(round(x1 + ((y_top-y1)*(x2-x1))/(y2-y1)))   
            
            # FOR AVG X-AXIS POSITIONS
            if x_btm <= mid_x_val:           # Left bottom region
                left_btm.append(x_btm)
            if x_btm > mid_x_val:            # Right bottom region
                right_btm.append(x_btm) 
            if x_top <= mid_x_val:           # Left top region
                left_top.append(x_top)
            if x_top > mid_x_val:            # Right top region
                right_top.append(x_top)
            
    # Calculate and draw lines with AVG positions:
    left_btm_avg = int(sum(left_btm)/(len(left_btm)))
    right_btm_avg = int(sum(right_btm)/(len(right_btm)))
    left_top_avg = int(sum(left_top)/(len(left_top)))
    right_top_avg = int(sum(right_top)/(len(right_top)))
    
    cv2.line(img, (left_btm_avg, y_btm), (left_top_avg, y_top), color, thickness)
    cv2.line(img, (right_btm_avg, y_btm), (right_top_avg, y_top), color, thickness)

def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):
    """
    `img` should be the output of a Canny transform.
        
    Returns an image with hough lines drawn.
    """
    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)
    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)
    draw_lines(line_img, lines)
    return line_img

# Python 3 has support for cool math symbols.

def weighted_img(img, initial_img, α=0.8, β=1., λ=0.):
    """
    `img` is the output of the hough_lines(), An image with lines drawn on it.
    Should be a blank image (all black) with lines drawn on it.
    
    `initial_img` should be the image before any processing.
    
    The result image is computed as follows:
    
    initial_img * α + img * β + λ
    NOTE: initial_img and img must be the same shape!
    """
    return cv2.addWeighted(initial_img, α, img, β, λ)
```

## Test Images

Build your pipeline to work on the images in the directory "test_images"  
**You should make sure your pipeline works well on these images before you try the videos.**


```python
import os
os.listdir("test_images/")
```




    ['solidWhiteCurve.jpg',
     'solidWhiteRight.jpg',
     'solidYellowCurve.jpg',
     'solidYellowCurve2.jpg',
     'solidYellowLeft.jpg',
     'whiteCarLaneSwitch.jpg',
     'YellowUnderShade.jpg',
     'YellowUnderShade2.jpg',
     'YellowWhite.jpg',
     'YellowWhite2.jpg']



## Build a Lane Finding Pipeline



Build the pipeline and run your solution on all test_images. Make copies into the test_images directory, and you can use the images in your writeup report.

Try tuning the various parameters, especially the low and high Canny thresholds as well as the Hough lines parameters.


```python
# Applies the Grayscale transform
def grayscale(img):
    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)

# Applies a Gaussian Noise kernel
def gaussian_blur(img, kernel_size):
    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)

# Applies the Canny transform
def canny(img, low_threshold, high_threshold):
    return cv2.Canny(img, low_threshold, high_threshold)

# Applies an image mask.
# Only keeps the region of the image defined by the polygon
# formed from `vertices`. The rest of the image is set to black.
def region_of_interest(img, vertices):
    # defining a blank mask to start with
    mask = np.zeros_like(img)   
    
    # defining a 3 channel or 1 channel color to fill the mask with depending on the input image
    if len(img.shape) > 2:
        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image
        ignore_mask_color = (255,) * channel_count
    else:
        ignore_mask_color = 255
        
    # filling pixels inside the polygon defined by "vertices" with the fill color    
    cv2.fillPoly(mask, vertices, ignore_mask_color)
    # returning the image only where mask pixels are nonzero
    masked_image = cv2.bitwise_and(img, mask)
    return masked_image

# Returns an image with Hough lines drawn.
# `img` should be the output of a Canny transform.
def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):
    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)
    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)
    draw_lines(line_img, lines)
    return line_img

# Draws `lines` with `color` and `thickness`
def draw_lines(img, lines, color=[255, 0, 0], thickness=10):
    # For calculating avg x-axis values for lines
    mid_x_val = 485           # middle x-point diveds left and right lines
    left_btm = []
    left_top = []
    right_btm = []
    right_top = []
    
    for line in lines:
        for x1,y1,x2,y2 in line:
            # x-extrapolation formula:
            # x = x1 + ((y-y1)*(x2-x1))/(y2-y1)
            
            # Preventing case with divide by 0
            if y1 == y2: break
            
            # Apply x-extrapolation
            y_btm = img.shape[0]
            x_btm = int(round(x1 + ((y_btm-y1)*(x2-x1))/(y2-y1)))
            y_top = 320
            x_top = int(round(x1 + ((y_top-y1)*(x2-x1))/(y2-y1)))   
            
            # Preventing False detections
            if x_btm < 50:  x_btm = 70
            if x_btm > 900: x_btm = 900
            if x_top < 430: x_top = 450
            if x_top > 530: x_top = 500
            
            # FOR AVG X-AXIS POSITIONS
            if x_btm <= mid_x_val:           # Left bottom region
                left_btm.append(x_btm)
            if x_btm > mid_x_val:            # Right bottom region
                right_btm.append(x_btm) 
            if x_top <= mid_x_val:           # Left top region
                left_top.append(x_top)
            if x_top > mid_x_val:            # Right top region
                right_top.append(x_top)

    # Calculate and draw lines with AVG positions:
    # Preventing case with divide by 0
    len_lb = len(left_btm)
    if len_lb == 0: len_lb = 1
        
    len_rb = len(right_btm)
    if len_rb == 0: len_rb = 1
        
    len_lt = len(left_top)
    if len_lt == 0: len_lt = 1
        
    len_rt = len(right_top)
    if len_rt == 0: len_rt = 1
    
    # AVG values for lines coordinates
    left_btm_avg = int(sum(left_btm)/(len_lb))
    right_btm_avg = int(sum(right_btm)/(len_rb))
    left_top_avg = int(sum(left_top)/(len_lt))
    right_top_avg = int(sum(right_top)/(len_rt))
    
    cv2.line(img, (left_btm_avg, y_btm), (left_top_avg, y_top), color, thickness)
    cv2.line(img, (right_btm_avg, y_btm), (right_top_avg, y_top), color, thickness)

# The result image is computed as follows:
# initial_img * α + img * β + λ
# NOTE: initial_img and img must be the same shape!
def weighted_img(img, initial_img, alpha=0.8, beta=1., gamma=0.):
    return cv2.addWeighted(initial_img, alpha, img, beta, gamma)

def process_image(image):
    # NOTE: The output should be a color image (3 channel) for processing video below
    
    # 1. Video adjusment:
    height, width = image.shape[:2]
    if height != 540 or width != 960:
        height = 540
        width = 960
        thumbnail = cv2.resize(image, (width, height), interpolation = cv2.INTER_CUBIC)
        gray_image = grayscale(thumbnail)     # apply grayscale to resized input image
        image = thumbnail                     # save resized original (input image)
    else:
        # 2. Apply grayscale transform
        gray_image = grayscale(image)
    
    # 3. Apply Gaussian smoothing / blurring
    kernel_size = 3        # Must be an odd number (3, 5, 7...)
    blur_gray_image = gaussian_blur(gray_image, kernel_size)

    # 4. Define our parameters for Canny and apply
    low_threshold = 50
    high_threshold = 150
    canny_image = canny(blur_gray_image, low_threshold, high_threshold)

    # 5. Define image mask (polygon of interest)
    imshape = image.shape
    vertices = np.array([[(60, imshape[0]), (480, 300), (490, 300), (imshape[1]-60, imshape[0])]], dtype=np.int32)
    masked_image = region_of_interest(canny_image, vertices)

    # 6.1 Define the Hough transform parameters
    rho = 2                # distance resolution in pixels of the Hough grid
    theta = np.pi/180      # angular resolution in radians of the Hough grid
    threshold = 30         # minimum number of votes (intersections in Hough grid cell)
    min_line_length = 100  # minimum number of pixels making up a line
    max_line_gap = 160     # maximum gap in pixels between connectable line segments
    # 6.2 Run Hough on edge detected image
    line_img = hough_lines(masked_image, rho, theta, threshold, min_line_length, max_line_gap)

    # 7. Get result image
    result_image = weighted_img(line_img, image)

    return result_image
```

## Test on Videos

You know what's cooler than drawing lanes over images? Drawing lanes over video!

We can test our solution on two provided videos:


```python
# Import everything needed to edit/save/watch video clips
from moviepy.editor import VideoFileClip
from IPython.display import HTML
```

Let's try the one with the solid white lane on the right first ...


```python
import os 

# used by both images (here) and videos (next section).
def process_image(image):
    # NOTE: The output you return should be a color image (3 channel) for processing video below
    # TODO: put your pipeline here,
    # you should return the final output (image with lines are drawn on lanes)

    # Define a kernel size for Gaussian smoothing / blurring
    kernel_size = 5 # must be odd
    
    # Define parameters for Canny edge detection
    # low to high ratio must be 1:2 or 1:3
    low_threshold = 50
    high_threshold = 150
    
    gray = grayscale(image)
    blur = gaussian_blur(gray, kernel_size) 
    edges = canny(blur, low_threshold, high_threshold)    
    
    imshape = image.shape
    ysize = imshape[0]
    xsize = imshape[1]
    
    vertices = np.array([[(0, ysize), (450, 320), (500, 320), (xsize, ysize)]], dtype=np.int32)    
    masked = region_of_interest(edges, vertices)
    
    rho = 2            # distance resolution in pixels of the Hough grid
    theta = np.pi/180  # angular resolution in radians of the Hough grid
    threshold = 20     # minimum number of votes (intersections in Hough grid cell)
    min_line_len = 100  # minimum number of pixels making up a line
    max_line_gap = 160 # maximum gap in pixels between connectable line segments
    line_image = hough_lines(masked, rho, theta, threshold, min_line_len, max_line_gap)
    
    result = weighted_img(line_image, image)
    
    # Create a "color" binary image to combine with line image
    return result

images = os.listdir("test_images/")
for file in images:
    
    # skip files starting with processed
    if file.startswith('processed'):
        continue
        
    image = mpimg.imread('test_images/' + file)   
    
    weighted = process_image(image)

    # plt.imshow(weighted)
    
    #break
    mpimg.imsave('test_images/processed-' + file, weighted)
    
    print("Procesed ", file)
```

    Procesed  solidWhiteCurve.jpg
    Procesed  solidWhiteRight.jpg
    Procesed  solidYellowCurve.jpg
    Procesed  solidYellowCurve2.jpg
    Procesed  solidYellowLeft.jpg
    Procesed  whiteCarLaneSwitch.jpg
    Procesed  YellowUnderShade.jpg
    Procesed  YellowUnderShade2.jpg
    Procesed  YellowWhite.jpg
    Procesed  YellowWhite2.jpg
    


```python
# Import everything needed to edit/save/watch video clips
from moviepy.editor import VideoFileClip
from IPython.display import HTML
```


```python
white_output = 'white.mp4'
clip1 = VideoFileClip("solidWhiteRight.mp4")
white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!
%time white_clip.write_videofile(white_output, audio=False)
```

    [MoviePy] >>>> Building video white.mp4
    [MoviePy] Writing video white.mp4
    

    100%|██████████████████████████████████████▊| 221/222 [00:01<00:00, 111.34it/s]
    

    [MoviePy] Done.
    [MoviePy] >>>> Video ready: white.mp4 
    
    Wall time: 2.25 s
    

Play the video inline, or if you prefer find the video in your filesystem (should be in the same directory) and play it in your video player of choice.


```python
HTML("""
<video width="960" height="540" controls>
  <source src="{0}">
</video>
""".format(white_output))
```





<video width="960" height="540" controls>
  <source src="white.mp4">
</video>



## Improve the draw_lines() function

**At this point, if you were successful with making the pipeline and tuning parameters, you probably have the Hough line segments drawn onto the road, but what about identifying the full extent of the lane and marking it clearly as in the example video (P1_example.mp4)?  Think about defining a line to run the full length of the visible lane based on the line segments you identified with the Hough Transform. As mentioned previously, try to average and/or extrapolate the line segments you've detected to map out the full extent of the lane lines. You can see an example of the result you're going for in the video "P1_example.mp4".**

**Go back and modify your draw_lines function accordingly and try re-running your pipeline. The new output should draw a single, solid line over the left lane line and a single, solid line over the right lane line. The lines should start from the bottom of the image and extend out to the top of the region of interest.**
Now for the one with the solid yellow lane on the left. This one's more tricky!


```python
yellow_output = 'yellow.mp4'
clip2 = VideoFileClip('solidYellowLeft.mp4')
yellow_clip = clip2.fl_image(process_image)
%time yellow_clip.write_videofile(yellow_output, audio=False)
```

    [MoviePy] >>>> Building video yellow.mp4
    [MoviePy] Writing video yellow.mp4
    

    100%|██████████████████████████████████████▉| 681/682 [00:06<00:00, 111.55it/s]
    

    [MoviePy] Done.
    [MoviePy] >>>> Video ready: yellow.mp4 
    
    Wall time: 6.36 s
    


```python
HTML("""
<video width="960" height="540" controls>
  <source src="{0}">
</video>
""".format(yellow_output))
```





<video width="960" height="540" controls>
  <source src="yellow.mp4">
</video>




## Writeup and Submission

If you're satisfied with your video outputs, it's time to make the report writeup in a pdf or markdown file. Once you have this Ipython notebook ready along with the writeup, it's time to submit for review! Here is a [link](https://github.com/udacity/CarND-LaneLines-P1/blob/master/writeup_template.md) to the writeup template file.


## Optional Challenge

Try your lane finding pipeline on the video below.  Does it still work?  Can you figure out a way to make it more robust?  If you're up for the challenge, modify your pipeline so it works with this video and submit it along with the rest of your project!


```python
challenge_output = 'extra.mp4'
clip2 = VideoFileClip('challenge.mp4')
challenge_clip = clip2.fl_image(process_image)
%time challenge_clip.write_videofile(challenge_output, audio=False)
```

    [MoviePy] >>>> Building video extra.mp4
    [MoviePy] Writing video extra.mp4
    

    100%|████████████████████████████████████████| 251/251 [00:04<00:00, 51.03it/s]
    

    [MoviePy] Done.
    [MoviePy] >>>> Video ready: extra.mp4 
    
    Wall time: 5.46 s
    


```python
HTML("""
<video width="960" height="540" controls>
  <source src="{0}">
</video>
""".format(challenge_output))
```





<video width="960" height="540" controls>
  <source src="extra.mp4">
</video>





```python

```
